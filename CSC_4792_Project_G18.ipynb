{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alpha-Male-Dennis/4792_Data-Mining-Group-18/blob/main/CSC_4792_Project_G18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5FUgw3wv5NK"
      },
      "source": [
        "# 1. Business Understanding\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "The Journal of Natural and Applied Sciences (JONAS) publishes multidisciplinary research articles across various disciplines, including environmental science, agriculture, mining, engineering, and water resources. Manually categorizing these articles into their respective disciplines based on titles and abstracts is time-consuming and subjective. An automated classification model is needed to accurately assign articles to their relevant disciplines using natural language processing (NLP) and machine learning techniques.\n",
        "\n",
        "## Business Objectives\n",
        "\n",
        "The primary business objectives are:\n",
        "\n",
        "1. Efficient Article Classification – Automate the categorization of journal articles to streamline editorial workflows.\n",
        "2. Improved Discoverability – Enhance search and retrieval of articles by discipline for researchers and readers.\n",
        "3. Reduced Manual Effort – Minimize the need for manual tagging by editors, reducing human error and workload.\n",
        "\n",
        "## Success Criteria\n",
        "* The model should correctly classify articles into predefined disciplines with high accuracy.\n",
        "* The solution should be scalable to handle new articles as the journal continues publishing.\n",
        "* The classification system should be interpretable, allowing editors to verify and adjust categories if needed.\n",
        "\n",
        "## Data Mining Goals\n",
        "\n",
        "* Text Preprocessing – Clean and preprocess article titles and abstracts (tokenization, stopword removal, stemming/lemmatization).\n",
        "* Feature Extraction – Convert text into numerical features using techniques like TF-IDF or word embeddings (Word2Vec, GloVe).\n",
        "* Model Development – Implement a supervised classification model (e.g., Naïve Bayes, SVM, Random Forest, or Neural Networks) to predict article disciplines.\n",
        "* Evaluation – Assess model performance using metrics such as accuracy, precision, recall, and F1-score.\n",
        "\n",
        "## Initial Project Success Criteria\n",
        "\n",
        "* Accuracy: The model should achieve at least 85% accuracy in classifying articles into the correct discipline.\n",
        "* Interpretability: The model should provide explainable predictions (e.g., feature importance in decision-making).\n",
        "* Scalability: The solution should handle new, unseen articles without significant performance degradation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi6I54bzdbu7"
      },
      "source": [
        "# 2. Data Understanding\n",
        "\n",
        "## First mount drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwGLs-yNxdAz"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOL4-KE7SK2I"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS_N_lFVTDD9"
      },
      "source": [
        "## 2.1 Load Raw Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-Jl9enLTE1_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/JONAS dataset/journal_articles_final.csv')\n",
        "print(\"Dataset loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5oeYYmtXkXt"
      },
      "source": [
        "## 2.2 Initial Data Exploration\n",
        "\n",
        "### Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_6HsisRX0-a"
      },
      "outputs": [],
      "source": [
        "# Display the first 5 rows\n",
        "print(\"First 5 rows:\")\n",
        "display(df.head())\n",
        "\n",
        "# Dataset shape (number of articles, columns)\n",
        "print(\"\\nDataset shape:\", df.shape)\n",
        "\n",
        "# Column names and data types\n",
        "print(\"\\nData types and non-null counts:\")\n",
        "display(df.info())\n",
        "\n",
        "# Summary statistics for numerical columns (if any)\n",
        "print(\"\\nSummary statistics:\")\n",
        "display(df.describe(include='all'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCteDTSqX5eC"
      },
      "source": [
        "### Discipline Distribution (Bar Chart)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c4obp80X-5t"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot distribution of disciplines\n",
        "plt.figure(figsize=(10, 6))\n",
        "df['discipline'].value_counts().plot(kind='bar', color='skyblue')\n",
        "plt.title('Distribution of Articles by Discipline')\n",
        "plt.xlabel('discipline')\n",
        "plt.ylabel('Number of Articles')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXn5kU1vSfqn"
      },
      "source": [
        "## 2.3 Text Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liXOVoCvSkU2"
      },
      "source": [
        "### Word Frequency Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IS_AAkr_Sm4z"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Combine all abstracts into a single string\n",
        "all_text = ' '.join(df['abstract'].astype(str)).lower()\n",
        "\n",
        "# Tokenize and remove stopwords\n",
        "tokens = [word for word in all_text.split() if word not in stopwords.words('english')]\n",
        "word_freq = Counter(tokens).most_common(20)\n",
        "\n",
        "# Plot top 20 frequent words\n",
        "plt.figure(figsize=(10, 6))\n",
        "pd.DataFrame(word_freq, columns=['Word', 'Frequency']).plot(x='Word', y='Frequency', kind='bar', color='teal')\n",
        "plt.title('Top 20 Frequent Words (Excluding Stopwords)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbKNEgu4kUbs"
      },
      "source": [
        "##2.4 Data Quality Verification\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUCx5OEVrJnf"
      },
      "source": [
        "### Missing Values Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iu7FCtWKkycu"
      },
      "outputs": [],
      "source": [
        "print(\"Missing values per column:\")\n",
        "display(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr1PCZkjtPk_"
      },
      "source": [
        "### Duplicates Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lJHYizftfed"
      },
      "outputs": [],
      "source": [
        "print(\"Number of duplicate articles:\", df.duplicated(subset=['title', 'abstract']).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83ak8uXyuuXA"
      },
      "source": [
        "## 2.5 Initial Findings Summary\n",
        "\n",
        "### Data Loading and Description:\n",
        "\n",
        "The dataset, journal_articles_final.csv, was successfully loaded into a pandas DataFrame named df.\n",
        "The dataset contains 47 articles and 7 columns: article_id, title, abstract, keywords, discipline, year, and volume.\n",
        "The columns have the following data types: article_id, title, abstract, keywords, and discipline are objects (strings), while year and volume are integers.\n",
        "\n",
        "### Data Exploration:\n",
        "\n",
        "The distribution of articles by discipline shows that \"Engineering\" has the highest number of articles, followed by \"Environmental Science\" and \"Agriculture\".\n",
        "The top 20 most frequent words in the abstracts (excluding stopwords) include terms like \"study\", \"data\", \"water\", \"mining\", and \"construction\", which align with the prominent disciplines in the dataset.\n",
        "\n",
        "### Data Quality Verification:\n",
        "\n",
        "There are missing values in the abstract (2 missing) and keywords (5 missing) columns.\n",
        "There are no duplicate articles based on the combination of 'title' and 'abstract'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PGMFOw3xhUj"
      },
      "source": [
        "# 3. Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsbgKUM9xo0z"
      },
      "source": [
        "## 3.1 Setup and Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wS8--OPxqJY"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data for text preprocessing\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Mount Google Drive to access our datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Load the Dataset ---\n",
        "# !!! IMPORTANT: Change this path to the location of the file in your Google Drive !!!\n",
        "var_jonas_dataframe = pd.read_csv('/content/drive/MyDrive/JONAS dataset/journal_articles_final.csv')\n",
        "\n",
        "print(\"Setup complete. Raw student performance dataset loaded.\")\n",
        "# We will work with a copy to keep the original raw data intact\n",
        "var_working_df = var_jonas_dataframe.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zC47pj_zOkq"
      },
      "source": [
        "## 3.2 Data Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFlgV1gDzY2Y"
      },
      "outputs": [],
      "source": [
        "# Define the list of columns we believe are most relevant for our initial model\n",
        "var_relevant_columns = [\n",
        "    'article_id', 'title', 'abstract', 'keywords', 'discipline'\n",
        "]\n",
        "\n",
        "# Select only these columns from the working DataFrame\n",
        "var_selected_dataframe = var_jonas_dataframe[var_relevant_columns]\n",
        "\n",
        "print(\"--- Data Selection Complete ---\")\n",
        "print(f\"Original number of columns: {len(var_jonas_dataframe.columns)}\")\n",
        "print(f\"Number of columns after selection: {len(var_selected_dataframe.columns)}\")\n",
        "var_selected_dataframe.head(2).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G7ewWN81Fk2"
      },
      "source": [
        "## 3.3 Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdNgXGMq1Ps2"
      },
      "source": [
        "### 3.3.1 Handling Missing Values (General)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vD2AxRjn1aug"
      },
      "outputs": [],
      "source": [
        "# Step 1: Identify missing values in our selected data\n",
        "print(\"--- Missing Values Before Preprocessing ---\")\n",
        "var_missing_counts = var_selected_dataframe.isnull().sum()\n",
        "print(var_missing_counts[var_missing_counts > 0]) # Show only columns with missing data\n",
        "\n",
        "# Step 2: Define and apply an imputation strategy\n",
        "# For text columns (title, abstract, keywords), we'll fill missing values with empty strings\n",
        "var_text_cols = ['title', 'abstract', 'keywords']\n",
        "\n",
        "for var_col in var_text_cols:\n",
        "    var_selected_dataframe[var_col].fillna('', inplace=True)\n",
        "\n",
        "# For the discipline column (our target variable), drop rows with missing values\n",
        "# because we cannot classify articles without knowing their discipline.\n",
        "var_selected_dataframe.dropna(subset=['discipline'], inplace=True)\n",
        "\n",
        "# Step 3: Verify that the missing values have been handled\n",
        "print(\"\\n--- Missing Values After Preprocessing ---\")\n",
        "var_missing_after = var_selected_dataframe.isnull().sum()\n",
        "print(var_missing_after[var_missing_after > 0])\n",
        "\n",
        "# Step 4: Additional preprocessing for text data\n",
        "# Clean text data by removing extra whitespace and ensuring consistent formatting\n",
        "var_selected_dataframe['title'] = var_selected_dataframe['title'].str.strip()\n",
        "var_selected_dataframe['abstract'] = var_selected_dataframe['abstract'].str.strip()\n",
        "var_selected_dataframe['keywords'] = var_selected_dataframe['keywords'].str.strip()\n",
        "\n",
        "# Step 5: Check the final shape of the dataset\n",
        "print(f\"\\n--- Final Dataset Shape ---\")\n",
        "print(f\"Rows: {var_selected_dataframe.shape[0]}, Columns: {var_selected_dataframe.shape[1]}\")\n",
        "\n",
        "# Display a sample of the preprocessed data\n",
        "print(\"\\n--- Sample of Preprocessed Data ---\")\n",
        "print(var_selected_dataframe.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQNMNWK3ppV3"
      },
      "source": [
        "### 3.3.2 Handling Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEU-S3U4p7QU"
      },
      "outputs": [],
      "source": [
        "var_num_duplicates_before = var_selected_dataframe.duplicated(subset=['title', 'abstract']).sum()\n",
        "print(f\"Number of duplicate articles found: {var_num_duplicates_before}\")\n",
        "\n",
        "var_preprocessed_dataframe = var_selected_dataframe.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
        "print(f\"Shape after removing duplicates: {var_preprocessed_dataframe.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMVQN_Zg-V1W"
      },
      "source": [
        "### 3.3.3 Text Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pESbs5Bd-b3N"
      },
      "outputs": [],
      "source": [
        "def fxn_convert_to_lowercase(var_text):\n",
        "    if isinstance(var_text, str):\n",
        "        return var_text.lower()\n",
        "    return \"\"\n",
        "\n",
        "def fxn_remove_punctuation(var_text):\n",
        "    if isinstance(var_text, str):\n",
        "        return \"\".join([var_char for var_char in var_text if var_char not in string.punctuation])\n",
        "    return \"\"\n",
        "\n",
        "def fxn_remove_stopwords(var_text):\n",
        "    if isinstance(var_text, str):\n",
        "        var_tokens = word_tokenize(var_text)\n",
        "        var_stop_words = set(stopwords.words('english'))\n",
        "        var_filtered_tokens = [var_word for var_word in var_tokens if var_word not in var_stop_words]\n",
        "        return \" \".join(var_filtered_tokens)\n",
        "    return \"\"\n",
        "\n",
        "def fxn_stem_text(var_text):\n",
        "    if isinstance(var_text, str):\n",
        "        var_tokens = word_tokenize(var_text)\n",
        "        var_stemmer = PorterStemmer()\n",
        "        var_stemmed_tokens = [var_stemmer.stem(var_word) for var_word in var_tokens]\n",
        "        return \" \".join(var_stemmed_tokens)\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def fxn_preprocess_text_pipeline(var_text):\n",
        "    if not isinstance(var_text, str):\n",
        "        return \"\"\n",
        "    var_processed_text = fxn_convert_to_lowercase(var_text)\n",
        "    var_processed_text = fxn_remove_punctuation(var_processed_text)\n",
        "    var_processed_text = fxn_remove_stopwords(var_processed_text)\n",
        "    var_processed_text = fxn_stem_text(var_processed_text)\n",
        "    return var_processed_text\n",
        "\n",
        "# Apply the preprocessing pipeline to the relevant text columns\n",
        "var_preprocessed_dataframe['CleanedTitle'] = var_preprocessed_dataframe['title'].apply(fxn_preprocess_text_pipeline)\n",
        "var_preprocessed_dataframe['CleanedAbstract'] = var_preprocessed_dataframe['abstract'].apply(fxn_preprocess_text_pipeline)\n",
        "var_preprocessed_dataframe['CleanedKeywords'] = var_preprocessed_dataframe['keywords'].apply(fxn_preprocess_text_pipeline)\n",
        "\n",
        "print(\"--- Text Pre-processing Complete ---\")\n",
        "# Display the original and cleaned columns for comparison\n",
        "display(var_preprocessed_dataframe[['title', 'CleanedTitle', 'abstract', 'CleanedAbstract', 'keywords', 'CleanedKeywords']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUdxEut8ADOo"
      },
      "source": [
        "### 3.3.4 Code See The Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhSbwi5dAGCx"
      },
      "outputs": [],
      "source": [
        "# Select a sample motivation to process (find a non-empty one)\n",
        "var_sample_text = var_preprocessed_dataframe['abstract'].dropna().iloc[0]\n",
        "\n",
        "print(f\"--- 1. ORIGINAL TEXT ---\\n'{var_sample_text}'\\n\")\n",
        "\n",
        "# Apply step 1: Lowercasing\n",
        "var_lowercase_text = fxn_convert_to_lowercase(var_sample_text)\n",
        "print(f\"--- 2. AFTER LOWERCASE ---\\n'{var_lowercase_text}'\\n\")\n",
        "\n",
        "# Apply step 2: Remove Punctuation\n",
        "var_no_punct_text = fxn_remove_punctuation(var_lowercase_text)\n",
        "print(f\"--- 3. AFTER REMOVING PUNCTUATION ---\\n'{var_no_punct_text}'\\n\")\n",
        "\n",
        "# Apply step 3: Remove Stopwords\n",
        "var_no_stopwords_text = fxn_remove_stopwords(var_no_punct_text)\n",
        "print(f\"--- 4. AFTER REMOVING STOPWORDS ---\\n'{var_no_stopwords_text}'\\n\")\n",
        "\n",
        "# Apply step 4: Stemming\n",
        "var_stemmed_text = fxn_stem_text(var_no_stopwords_text)\n",
        "print(f\"--- 5. FINAL STEMMED TEXT ---\\n'{var_stemmed_text}'\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GvUkT9iEGgH"
      },
      "source": [
        "## 3.4.1 Transforming Categorical Data (Encoding)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdT2Or9IExor"
      },
      "source": [
        "This will convert the categorical discipline names into a numerical format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EroxksenEVvG"
      },
      "outputs": [],
      "source": [
        "# Perform One-Hot Encoding on 'discipline'\n",
        "var_dummies_dataframe = pd.get_dummies(var_preprocessed_dataframe['discipline'], prefix='discipline')\n",
        "\n",
        "# Join the new dummy columns back to our main DataFrame\n",
        "var_transformed_dataframe = pd.concat([var_preprocessed_dataframe, var_dummies_dataframe], axis=1)\n",
        "\n",
        "print(\"--- DataFrame after One-Hot Encoding ---\")\n",
        "# Display the original column and the new binary columns\n",
        "display(var_transformed_dataframe[['discipline'] + [col for col in var_transformed_dataframe.columns if col.startswith('discipline_')]].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7527F9mHRyt"
      },
      "source": [
        "## 3.4.2 Transforming Text Data (Bag-of-Words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUxXBKAcHS-K"
      },
      "source": [
        "This section transforms the cleaned text data into numerical features using the Bag-of-Words model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qw9jqZTzHNaH"
      },
      "outputs": [],
      "source": [
        "# Feature Engineering\n",
        "# Combine the cleaned text columns into a single column\n",
        "var_preprocessed_dataframe['CombinedText'] = var_preprocessed_dataframe['CleanedTitle'] + ' ' + var_preprocessed_dataframe['CleanedAbstract'] + ' ' + var_preprocessed_dataframe['CleanedKeywords']\n",
        "\n",
        "\n",
        "# --- Complete Text Preprocessing Pipeline ---\n",
        "def fxn_preprocess_text_pipeline(var_text):\n",
        "    \"\"\"A complete text preprocessing pipeline for journal articles.\"\"\"\n",
        "    if not isinstance(var_text, str):\n",
        "        return \"\"  # Return empty string for non-string (e.g., NaN) inputs\n",
        "\n",
        "    var_processed_text = fxn_convert_to_lowercase(var_text)\n",
        "    var_processed_text = fxn_remove_punctuation(var_processed_text)\n",
        "    var_processed_text = fxn_remove_stopwords(var_processed_text)\n",
        "    var_processed_text = fxn_stem_text(var_processed_text)\n",
        "\n",
        "    return var_processed_text\n",
        "\n",
        "# --- Apply Preprocessing to Journal Article Abstracts ---\n",
        "\n",
        "# Step 1: Handle missing values in the abstract column\n",
        "var_abstract_series = df['abstract'].fillna('')\n",
        "\n",
        "# Step 2: Apply the full cleaning pipeline to the abstracts\n",
        "var_cleaned_abstracts = var_abstract_series.apply(fxn_preprocess_text_pipeline)\n",
        "\n",
        "# Step 3: Transform the cleaned text into a Bag-of-Words representation\n",
        "var_vectorizer = CountVectorizer(max_features=1000)  # Limit to top 1000 features\n",
        "var_bow_matrix = var_vectorizer.fit_transform(var_preprocessed_dataframe['CombinedText'])\n",
        "\n",
        "# Step 4: Convert the result into a DataFrame for easy viewing\n",
        "var_bow_dataframe = pd.DataFrame(var_bow_matrix.toarray(),\n",
        "                                 columns=var_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"--- Bag-of-Words Transformation for Journal Abstracts ---\")\n",
        "print(\"Original text data shape:\", var_preprocessed_dataframe['CombinedText'].shape)\n",
        "print(\"Transformed BoW data shape:\", var_bow_dataframe.shape)\n",
        "print(\"\\nSample of BoW DataFrame (showing word counts per article):\")\n",
        "display(var_bow_dataframe.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjQ2dnigV00Q"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we have successfully completed the data preparation phase for classifying journal articles by discipline. We started by loading and exploring the dataset to understand its structure and identify initial data quality issues.\n",
        "\n",
        "The key steps undertaken in this phase were:\n",
        "\n",
        "1.  **Data Cleaning:** We handled missing values in the 'keywords' column by filling them with empty strings and verified that there were no duplicate articles based on title and abstract.\n",
        "2.  **Feature Engineering:** We created a combined text feature by concatenating the cleaned 'title', 'abstract', and 'keywords' to provide a comprehensive text representation for each article.\n",
        "3.  **Data Transformation:** We converted the categorical 'discipline' labels into a numerical format using One-Hot Encoding and transformed the combined text data into a numerical Bag-of-Words representation.\n",
        "\n",
        "The data is now cleaned, features have been engineered, and the data is transformed into a format suitable for input into machine learning classification models. The next steps will involve selecting, training, and evaluating different models to determine the best approach for automated article classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmRdAYFdItCP"
      },
      "source": [
        "#4. Modelling\n",
        "\n",
        "## Algorithm Selection\n",
        "For this academic paper classification project, we selected three different classification algorithms to compare their performance:\n",
        "1. Random Forest Classifier\n",
        "Why chosen: Excellent for handling high-dimensional data like text embeddings, provides feature importance insights, and is robust against overfitting through its ensemble approach.\n",
        "\n",
        "2. Support Vector Machine (SVM)\n",
        "Why chosen: Particularly effective in high-dimensional spaces (like our 384-dimensional embeddings), works well with linear decision boundaries, and handles complex classification tasks.\n",
        "\n",
        "3. Logistic Regression\n",
        "Why chosen: A strong baseline classifier that works well with dense numerical features like embeddings, provides probability estimates, and is computationally efficient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfW1wRykIuII"
      },
      "source": [
        "## Data Preparation and Splitting\n",
        "\n",
        "### Dataset Characteristics:\n",
        "1. Total samples: 46 research papers\n",
        "\n",
        "2. Number of classes: 14 different academic disciplines\n",
        "\n",
        "Class distribution: Highly imbalanced (Mining Engineering: 14 samples, some disciplines: only 1 sample)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byIU_S98Iwu3"
      },
      "source": [
        "### Splitting Strategy:\n",
        "1. Test size: 25% of data (12 samples)\n",
        "\n",
        "2. Training size: 75% of data (34 samples)\n",
        "\n",
        "3. Challenge: Attempted stratified splitting to maintain class proportions, but this failed due to classes with only 1 member\n",
        "\n",
        "#### Solution: Used non-stratified random splitting with random_state=42 for reproducibility\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQU3uyv5I1bY"
      },
      "source": [
        "### Feature Engineering:\n",
        "\n",
        "1. Used Sentence-BERT embeddings ('all-MiniLM-L6-v2') to convert text to 384-dimensional numerical vectors\n",
        "\n",
        "2. Combined title and abstract for comprehensive document representation\n",
        "\n",
        "3. Encoded target labels using LabelEncoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAzqrORVI2MC"
      },
      "source": [
        "### Initial Results\n",
        "\n",
        "####The LOOCV accuracy scores on the training set (34 samples) were:\n",
        "\n",
        "* Random Forest: 14.7% accuracy\n",
        "\n",
        "* SVM: 26.5% accuracy\n",
        "\n",
        "* Logistic Regression: 32.4% accuracy\n",
        "\n",
        "####Observations:\n",
        "\n",
        "* The low scores are expected due to the small dataset, high dimensionality (384 features), many classes (14) with severe imbalance, and the complexity of text classification.\n",
        "\n",
        "* Logistic Regression performed best, which is typical for limited data because it estimates fewer parameters than more complex models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOV8hclGqoEe"
      },
      "source": [
        "###Modelling code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1PLUGmJqscu"
      },
      "outputs": [],
      "source": [
        "# ===== MODELING =====\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"MODELING PHASE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Import additional libraries needed for modeling and embeddings\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "# Removed TfidfVectorizer as we will use embeddings\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "# Import libraries for Sentence Embeddings\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch # Import torch to check for GPU availability\n",
        "\n",
        "\n",
        "# 1. Select appropriate data mining algorithms\n",
        "\"\"\"\n",
        "We'll use three classification algorithms:\n",
        "1. Random Forest - Excellent for text classification, handles high dimensionality well,\n",
        "   provides feature importance, and is robust to overfitting. Works with dense embeddings.\n",
        "2. Support Vector Machine (SVM) - Effective in high-dimensional spaces like text data.\n",
        "   Works very well with dense embeddings.\n",
        "3. Multinomial Naive Bayes - Traditionally strong for text classification tasks,\n",
        "   efficient with large feature sets. Note: MultinomialNB expects non-negative integer features (like counts),\n",
        "   so it's generally not suitable for dense embeddings. We will replace it or use a different variant.\n",
        "   Let's replace MultinomialNB with a different model that handles dense data, like a simple Logistic Regression.\n",
        "\"\"\"\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "# Prepare the data for modeling\n",
        "print(\"Preparing data for modeling...\")\n",
        "\n",
        "# Use the preprocessed data\n",
        "var_model_df = var_preprocessed_dataframe.copy()\n",
        "\n",
        "# Check if we have the cleaned text columns\n",
        "if 'CleanedTitle' not in var_model_df.columns or 'CleanedAbstract' not in var_model_df.columns:\n",
        "    print(\"Error: Text preprocessing not completed. Please run the text preprocessing section first.\")\n",
        "else:\n",
        "    # Combine text features\n",
        "    # Using 'enhanced_text' which is less aggressively processed might be better for embeddings\n",
        "    # var_model_df['CombinedText'] = var_model_df['CleanedTitle'] + \" \" + var_model_df['CleanedAbstract'] + \" \" + var_model_df['CleanedKeywords']\n",
        "    # Let's use the original title and abstract for embeddings, as they are designed for more natural text\n",
        "    var_model_df['CombinedText'] = var_model_df['title'] + \" \" + var_model_df['abstract']\n",
        "\n",
        "\n",
        "    # Analyze class distribution (for information, not filtering)\n",
        "    class_distribution = var_model_df['discipline'].value_counts()\n",
        "    print(\"Class distribution (using all data):\")\n",
        "    display(class_distribution)\n",
        "\n",
        "    # --- Using all 46 samples ---\n",
        "    # using all available data\n",
        "\n",
        "    # Encode the target variable\n",
        "    label_encoder = LabelEncoder()\n",
        "    var_y_encoded = label_encoder.fit_transform(var_model_df['discipline'])\n",
        "    class_names = label_encoder.classes_\n",
        "\n",
        "    print(f\"\\nNumber of classes: {len(class_names)}\")\n",
        "    print(\"Classes:\", class_names)\n",
        "    print(f\"Total samples: {len(var_model_df)}\") # Now this will be 46\n",
        "\n",
        "\n",
        "    # 2. Generate Sentence Embeddings\n",
        "    print(\"\\nGenerating Sentence Embeddings...\")\n",
        "\n",
        "    # Choose a pre-trained Sentence-BERT model\n",
        "    # 'all-MiniLM-L6-v2' is a good balance of size and performance\n",
        "    # Check for GPU availability\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "\n",
        "    # Generate embeddings for the combined text\n",
        "    # Process in batches if necessary for large datasets, but 46 is small\n",
        "    var_X = embedding_model.encode(var_model_df['CombinedText'].tolist(), show_progress_bar=True)\n",
        "\n",
        "    print(f\"Sentence embeddings generated. Shape: {var_X.shape}\")\n",
        "\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    print(\"\\nSplitting data into training and testing sets...\")\n",
        "\n",
        "    # Adjust test size or splitting strategy for the full, imbalanced dataset\n",
        "    # Given the very small size (46) and many classes, stratified split is preferred if possible.\n",
        "    test_size = 0.25 # Adjusted test size\n",
        "\n",
        "    try:\n",
        "        # Use Stratified train_test_split if possible\n",
        "        var_X_train, var_X_test, var_y_train, var_y_test = train_test_split(\n",
        "            var_X, var_y_encoded, test_size=test_size, random_state=42, stratify=var_y_encoded\n",
        "        )\n",
        "        print(f\"Using Stratified train_test_split with test size: {test_size:.2f}\")\n",
        "\n",
        "    except ValueError as e:\n",
        "         print(f\"Could not perform stratified split ({e}). Trying non-stratified split.\")\n",
        "         # Fallback to non-stratified if stratified fails (due to very small class sizes in test split)\n",
        "         var_X_train, var_X_test, var_y_train, var_y_test = train_test_split(\n",
        "             var_X, var_y_encoded, test_size=test_size, random_state=42, stratify=None # Explicitly set stratify=None\n",
        "         )\n",
        "         print(f\"Using Non-Stratified train_test_split with test size: {test_size:.2f}\")\n",
        "\n",
        "\n",
        "    print(f\"Training set size: {var_X_train.shape[0]}\")\n",
        "    print(f\"Testing set size: {var_X_test.shape[0]}\")\n",
        "    print(f\"Number of features (embedding dimensions): {var_X_train.shape[1]}\")\n",
        "\n",
        "\n",
        "    # 3. Train the chosen models\n",
        "    print(\"\\nTraining models...\")\n",
        "\n",
        "    # Initialize the models suitable for dense embeddings and imbalance\n",
        "    models = {\n",
        "        'Random Forest': RandomForestClassifier(\n",
        "            n_estimators=50,\n",
        "            random_state=42,\n",
        "            class_weight='balanced_subsample', # Use balanced_subsample for better handling with small samples\n",
        "            max_depth=5 # Limit depth to prevent overfitting\n",
        "        ),\n",
        "        'SVM': SVC(\n",
        "            kernel='linear',\n",
        "            random_state=42,\n",
        "            probability=True,\n",
        "            C=1.0,\n",
        "            class_weight='balanced'\n",
        "        ),\n",
        "        'Logistic Regression': LogisticRegression( # Replaced Naive Bayes\n",
        "             random_state=42,\n",
        "             max_iter=1000, # Increase max_iter for convergence\n",
        "             class_weight='balanced'\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # Train each model and store results\n",
        "    trained_models = {}\n",
        "    training_results = {}\n",
        "\n",
        "    # Use appropriate cross-validation strategy\n",
        "    # With very small and imbalanced classes, LOOCV might be more informative\n",
        "    from sklearn.model_selection import LeaveOneOut\n",
        "    cv = LeaveOneOut()\n",
        "    print(\"\\nUsing Leave-One-Out cross-validation for robust evaluation on small dataset.\")\n",
        "\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name}...\")\n",
        "        model.fit(var_X_train, var_y_train) # Train on the training split\n",
        "\n",
        "        trained_models[name] = model\n",
        "\n",
        "        # Cross-validation to assess training performance using LOOCV\n",
        "        try:\n",
        "            # Use the training data for LOOCV\n",
        "            cv_scores = cross_val_score(model, var_X_train, var_y_train, cv=cv, scoring='accuracy')\n",
        "            training_results[name] = {\n",
        "                'cv_mean_accuracy': cv_scores.mean(),\n",
        "                'cv_std_accuracy': cv_scores.std(),\n",
        "                'cv_scores': cv_scores\n",
        "            }\n",
        "            print(f\"  {name} LOOCV Accuracy (on Training Set): {cv_scores.mean():.3f}\") # LOOCV std is not meaningful\n",
        "        except Exception as e:\n",
        "            print(f\"  Could not perform cross-validation for {name}: {e}\")\n",
        "            training_results[name] = {\n",
        "                'cv_mean_accuracy': np.nan,\n",
        "                'cv_std_accuracy': np.nan,\n",
        "                'cv_scores': []\n",
        "            }\n",
        "\n",
        "\n",
        "    print(\"\\nModel training completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the Models"
      ],
      "metadata": {
        "id": "WDksaonYsxvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "output_dir = \"/content/drive/MyDrive/JONAS dataset/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save models and encoder\n",
        "print(\"\\nSaving models and encoder to Google Drive...\")\n",
        "\n",
        "# Save the label encoder (needed to decode predictions)\n",
        "joblib.dump(label_encoder, os.path.join(output_dir, \"label_encoder_final_g18.pkl\"))\n",
        "\n",
        "# Save each trained model\n",
        "for name, model in trained_models.items():\n",
        "    filename = os.path.join(output_dir, f\"{name.replace(' ', '_').lower()}_model.pkl\")\n",
        "    joblib.dump(model, filename)\n",
        "    print(f\"  Saved {name} model to {filename}\")\n",
        "\n",
        "print(\"\\nAll models and encoder saved successfully!\")\n"
      ],
      "metadata": {
        "id": "j3D9vyRZsm5W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}